measure of differences
reference:https://www.zhihu.com/question/19734616
1.Pearson Correlation
2.Cosine
3.Standardization
4.Summed Square and 
5.Sample Variance
6.Euclidean Distance

about big data processing:https://blog.csdn.net/v_july_v/article/details/7382693

BGD,SGD,MBGD
reference:https://zhuanlan.zhihu.com/p/25765735

Newton's method and gradient descent:
reference:https://stats.stackexchange.com/questions/253632/why-is-newtons-method-not-widely-used-in-machine-learning
reference:https://blog.csdn.net/xbinworld/article/details/79113218?ref=myread
Gradient descent maximizes a function using knowledge of its derivative. 
Newton's method, a root finding algorithm, maximizes a function using knowledge of its second derivative. 
That can be faster when the second derivative is known and easy to compute (the Newton-Raphson algorithm is used in logistic regression). However, the analytic expression for the second derivative is often complicated or intractable, requiring a lot of computation. Numerical methods for computing the second derivative also require a lot of computation
-- if N values are required to compute the first derivative, N2 are required for the second derivative.


imbalanced data set:
https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/

L1 and L2
reference:
http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/
https://blog.csdn.net/YoYoDelphine/article/details/52888315
L1:sparse output, bulid-in feature selection,laplace distribution.
L2:no parse output and bulid-in feature selection, but improve overfitting,Gaussian distribution.
